{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#import spacy\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.svm import LinearSVC\n",
    "#from sklearn.svm import SVC\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('imdb/IMDB Dataset.csv')\n",
    "df['Label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49995</td>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49996</td>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49997</td>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49998</td>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49999</td>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment  Label\n",
       "0      One of the other reviewers has mentioned that ...  positive      1\n",
       "1      A wonderful little production. <br /><br />The...  positive      1\n",
       "2      I thought this was a wonderful way to spend ti...  positive      1\n",
       "3      Basically there's a family where a little boy ...  negative      0\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive      1\n",
       "...                                                  ...       ...    ...\n",
       "49995  I thought this movie did a down right good job...  positive      1\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative      0\n",
       "49997  I am a Catholic taught in parochial elementary...  negative      0\n",
       "49998  I'm going to have to disagree with the previou...  negative      0\n",
       "49999  No one expects the Star Trek movies to be high...  negative      0\n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.sentiment == 'positive','Label'] = 1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set.union(set(stopwords.words('english')),set(string.punctuation),set(['...','']))\n",
    "PUNCTUATION = set.union(set(string.punctuation),set(['...','....','']))\n",
    "PUNCTUATION.remove('.')\n",
    "WL = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PUNCTUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "model = BertForSequenceClassification.from_pretrained(pretrained_weights,\n",
    "                                                      output_hidden_states=True,\n",
    "                                                      output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████| 50000/50000 [00:10<00:00, 4996.96it/s]\n",
      "progress-bar: 100%|██████████| 50000/50000 [00:56<00:00, 887.26it/s] \n",
      "progress-bar: 100%|██████████| 50000/50000 [00:00<00:00, 163744.95it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean_txt(text):\n",
    "    text = text.lower()\n",
    "    text =  re.sub(r'<\\w+ />','',text)\n",
    "    text  = re.sub(r\"[^a-zA-Z0-9.]+\", ' ', text)\n",
    "    text = re.sub(r'\\.+', \".\", text)\n",
    "    text = re.sub(r'\\d+(th)?',' ',text) #remove all the numbers\n",
    "    text = re.sub(r'\\d+(th)?','number',text) #remove all the numbers\n",
    "    #text = re.sub(r'...',' ',text) #remove all the numbers\n",
    "    #text = re.sub(r'.',' [SEP] ',text) #remove all the numbers\n",
    "    #text = '[CLS] '+text+' [SEP]'\n",
    "    return text\n",
    "    \n",
    "def lemmatize(text): \n",
    "    tokens = text.split(' ')\n",
    "    return ' '.join([WL.lemmatize(word) for word in tokens])\n",
    "\n",
    "\n",
    "def add_special_tokens(text):\n",
    "    sentences = text.split('.')\n",
    "    if sentences[-1] == '':\n",
    "        sentences = sentences[:-1]\n",
    "    joined = ' [SEP] '.join(sentences)\n",
    "    return joined+' [SEP]'    \n",
    "    \n",
    "def clean_stopwords(text):\n",
    "    return tokens\n",
    "\n",
    "    \n",
    "df['cleaned'] = df.review.progress_apply(lambda r: clean_txt(r))\n",
    "df['lemmatized'] = df.cleaned.progress_apply(lambda r: lemmatize(r))\n",
    "df['cleaned'] = df.lemmatized.progress_apply(lambda r: add_special_tokens(r))\n",
    "\n",
    "#df['tokenized'] = df.cleaned.progress_apply(lambda r: tokenize_clean(r))\n",
    "#df['lemmatized'] = df.tokenized.progress_apply(lambda r: lemmatize(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsampled = df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "progress-bar:   0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      "progress-bar:   1%|▏         | 13/1000 [00:00<00:07, 128.97it/s]\u001b[A\n",
      "progress-bar:   2%|▏         | 24/1000 [00:00<00:08, 118.73it/s]\u001b[A\n",
      "progress-bar:   4%|▎         | 35/1000 [00:00<00:08, 113.97it/s]\u001b[A\n",
      "progress-bar:   4%|▍         | 44/1000 [00:00<00:09, 105.28it/s]\u001b[A\n",
      "progress-bar:   6%|▌         | 55/1000 [00:00<00:08, 105.16it/s]\u001b[A\n",
      "progress-bar:   7%|▋         | 66/1000 [00:00<00:08, 104.60it/s]\u001b[A\n",
      "progress-bar:   8%|▊         | 80/1000 [00:00<00:08, 113.18it/s]\u001b[A\n",
      "progress-bar:   9%|▉         | 91/1000 [00:00<00:08, 101.85it/s]\u001b[A\n",
      "progress-bar:  11%|█         | 107/1000 [00:00<00:07, 113.20it/s]\u001b[A\n",
      "progress-bar:  12%|█▏        | 120/1000 [00:01<00:07, 114.77it/s]\u001b[A\n",
      "progress-bar:  14%|█▎        | 136/1000 [00:01<00:07, 122.84it/s]\u001b[A\n",
      "progress-bar:  15%|█▍        | 149/1000 [00:01<00:08, 98.56it/s] \u001b[A\n",
      "progress-bar:  16%|█▋        | 163/1000 [00:01<00:07, 107.99it/s]\u001b[A\n",
      "progress-bar:  18%|█▊        | 175/1000 [00:01<00:07, 107.88it/s]\u001b[A\n",
      "progress-bar:  19%|█▊        | 187/1000 [00:01<00:07, 107.34it/s]\u001b[A\n",
      "progress-bar:  20%|█▉        | 199/1000 [00:01<00:07, 104.23it/s]\u001b[A\n",
      "progress-bar:  21%|██        | 210/1000 [00:01<00:07, 99.10it/s] \u001b[A\n",
      "progress-bar:  22%|██▏       | 221/1000 [00:02<00:08, 94.21it/s]\u001b[A\n",
      "progress-bar:  23%|██▎       | 231/1000 [00:02<00:08, 87.36it/s]\u001b[A\n",
      "progress-bar:  24%|██▍       | 242/1000 [00:02<00:08, 92.18it/s]\u001b[A\n",
      "progress-bar:  25%|██▌       | 252/1000 [00:02<00:09, 79.20it/s]\u001b[A\n",
      "progress-bar:  26%|██▋       | 264/1000 [00:02<00:08, 86.24it/s]\u001b[A\n",
      "progress-bar:  27%|██▋       | 274/1000 [00:02<00:08, 85.04it/s]\u001b[A\n",
      "progress-bar:  29%|██▉       | 288/1000 [00:02<00:07, 96.24it/s]\u001b[A\n",
      "progress-bar:  30%|██▉       | 299/1000 [00:02<00:08, 82.24it/s]\u001b[A\n",
      "progress-bar:  31%|███       | 309/1000 [00:03<00:09, 70.94it/s]\u001b[A\n",
      "progress-bar:  32%|███▏      | 319/1000 [00:03<00:08, 77.38it/s]\u001b[A\n",
      "progress-bar:  33%|███▎      | 330/1000 [00:03<00:08, 83.05it/s]\u001b[A\n",
      "progress-bar:  34%|███▍      | 339/1000 [00:03<00:07, 83.08it/s]\u001b[A\n",
      "progress-bar:  35%|███▍      | 348/1000 [00:03<00:07, 83.12it/s]\u001b[A\n",
      "progress-bar:  36%|███▌      | 360/1000 [00:03<00:06, 91.44it/s]\u001b[A\n",
      "progress-bar:  37%|███▋      | 370/1000 [00:03<00:07, 86.51it/s]\u001b[A\n",
      "progress-bar:  38%|███▊      | 381/1000 [00:03<00:06, 91.62it/s]\u001b[A\n",
      "progress-bar:  39%|███▉      | 391/1000 [00:04<00:07, 78.07it/s]\u001b[A\n",
      "progress-bar:  40%|████      | 400/1000 [00:04<00:07, 76.13it/s]\u001b[A\n",
      "progress-bar:  41%|████      | 411/1000 [00:04<00:07, 83.09it/s]\u001b[A\n",
      "progress-bar:  42%|████▏     | 420/1000 [00:04<00:06, 84.93it/s]\u001b[A\n",
      "progress-bar:  43%|████▎     | 433/1000 [00:04<00:06, 92.94it/s]\u001b[A\n",
      "progress-bar:  44%|████▍     | 443/1000 [00:04<00:05, 93.33it/s]\u001b[A\n",
      "progress-bar:  45%|████▌     | 454/1000 [00:04<00:05, 94.45it/s]\u001b[A\n",
      "progress-bar:  46%|████▋     | 465/1000 [00:04<00:05, 98.45it/s]\u001b[A\n",
      "progress-bar:  48%|████▊     | 476/1000 [00:05<00:05, 88.58it/s]\u001b[A\n",
      "progress-bar:  49%|████▊     | 486/1000 [00:05<00:05, 86.52it/s]\u001b[A\n",
      "progress-bar:  50%|████▉     | 498/1000 [00:05<00:05, 92.64it/s]\u001b[A\n",
      "progress-bar:  51%|█████     | 511/1000 [00:05<00:04, 101.07it/s]\u001b[A\n",
      "progress-bar:  52%|█████▏    | 523/1000 [00:05<00:04, 105.48it/s]\u001b[A\n",
      "progress-bar:  54%|█████▍    | 538/1000 [00:05<00:04, 114.64it/s]\u001b[A\n",
      "progress-bar:  55%|█████▌    | 550/1000 [00:05<00:03, 114.80it/s]\u001b[A\n",
      "progress-bar:  57%|█████▋    | 569/1000 [00:05<00:03, 129.61it/s]\u001b[A\n",
      "progress-bar:  58%|█████▊    | 583/1000 [00:05<00:03, 118.90it/s]\u001b[A\n",
      "progress-bar:  60%|█████▉    | 599/1000 [00:06<00:03, 128.10it/s]\u001b[A\n",
      "progress-bar:  61%|██████▏   | 614/1000 [00:06<00:02, 129.63it/s]\u001b[A\n",
      "progress-bar:  63%|██████▎   | 628/1000 [00:06<00:02, 129.40it/s]\u001b[A\n",
      "progress-bar:  64%|██████▍   | 642/1000 [00:06<00:02, 119.80it/s]\u001b[A\n",
      "progress-bar:  66%|██████▌   | 655/1000 [00:06<00:02, 121.65it/s]\u001b[A\n",
      "progress-bar:  67%|██████▋   | 668/1000 [00:06<00:02, 122.81it/s]\u001b[A\n",
      "progress-bar:  68%|██████▊   | 681/1000 [00:06<00:02, 118.61it/s]\u001b[A\n",
      "progress-bar:  69%|██████▉   | 694/1000 [00:06<00:02, 104.76it/s]\u001b[A\n",
      "progress-bar:  70%|███████   | 705/1000 [00:07<00:03, 82.00it/s] \u001b[A\n",
      "progress-bar:  72%|███████▏  | 715/1000 [00:07<00:03, 80.97it/s]\u001b[A\n",
      "progress-bar:  72%|███████▎  | 725/1000 [00:07<00:03, 85.69it/s]\u001b[A\n",
      "progress-bar:  74%|███████▎  | 735/1000 [00:07<00:02, 89.42it/s]\u001b[A\n",
      "progress-bar:  74%|███████▍  | 745/1000 [00:07<00:02, 92.33it/s]\u001b[A\n",
      "progress-bar:  76%|███████▌  | 755/1000 [00:07<00:02, 86.92it/s]\u001b[A\n",
      "progress-bar:  76%|███████▋  | 764/1000 [00:07<00:02, 87.64it/s]\u001b[A\n",
      "progress-bar:  77%|███████▋  | 773/1000 [00:07<00:02, 88.06it/s]\u001b[A\n",
      "progress-bar:  78%|███████▊  | 782/1000 [00:07<00:02, 88.24it/s]\u001b[A\n",
      "progress-bar:  80%|███████▉  | 797/1000 [00:08<00:02, 98.55it/s]\u001b[A\n",
      "progress-bar:  81%|████████  | 808/1000 [00:08<00:02, 90.72it/s]\u001b[A\n",
      "progress-bar:  82%|████████▏ | 818/1000 [00:08<00:01, 91.26it/s]\u001b[A\n",
      "progress-bar:  83%|████████▎ | 828/1000 [00:08<00:02, 64.22it/s]\u001b[A\n",
      "progress-bar:  84%|████████▎ | 836/1000 [00:08<00:02, 58.17it/s]\u001b[A\n",
      "progress-bar:  85%|████████▍ | 848/1000 [00:08<00:02, 68.60it/s]\u001b[A\n",
      "progress-bar:  86%|████████▋ | 863/1000 [00:08<00:01, 80.12it/s]\u001b[A\n",
      "progress-bar:  87%|████████▋ | 873/1000 [00:09<00:01, 85.15it/s]\u001b[A\n",
      "progress-bar:  88%|████████▊ | 884/1000 [00:09<00:01, 90.76it/s]\u001b[A\n",
      "progress-bar:  90%|████████▉ | 897/1000 [00:09<00:01, 94.70it/s]\u001b[A\n",
      "progress-bar:  91%|█████████ | 908/1000 [00:09<00:01, 90.29it/s]\u001b[A\n",
      "progress-bar:  92%|█████████▏| 919/1000 [00:09<00:00, 93.60it/s]\u001b[A\n",
      "progress-bar:  93%|█████████▎| 929/1000 [00:09<00:00, 90.65it/s]\u001b[A\n",
      "progress-bar:  94%|█████████▍| 940/1000 [00:09<00:00, 95.40it/s]\u001b[A\n",
      "progress-bar:  95%|█████████▌| 950/1000 [00:09<00:00, 96.22it/s]\u001b[A\n",
      "progress-bar:  96%|█████████▌| 960/1000 [00:09<00:00, 86.60it/s]\u001b[A\n",
      "progress-bar:  97%|█████████▋| 972/1000 [00:10<00:00, 93.37it/s]\u001b[A\n",
      "progress-bar: 100%|██████████| 1000/1000 [00:10<00:00, 97.17it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "def tokenize_pad(text,max_len=512):\n",
    "    tokenized = tokenizer.tokenize(text)\n",
    "    if len(tokenized) > max_len: \n",
    "        tokenized = tokenized[:max_len]\n",
    "    encoded = tokenizer.encode(tokenized,max_length=max_len)\n",
    "    size = len(encoded)\n",
    "    if len(encoded) < max_len:\n",
    "        encoded = encoded+[0]*(max_len-size)\n",
    "    return encoded\n",
    "    \n",
    "dfsampled['encoded'] = dfsampled.cleaned.progress_apply(tokenize_pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2023,  5292,  ...,     0,     0,     0],\n",
       "        [  101,  2203, 27242,  ...,     0,     0,     0],\n",
       "        [  101,  1045,  3427,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  2023,  2003,  ...,     0,     0,     0],\n",
       "        [  101,  1045,  2031,  ...,     0,     0,     0],\n",
       "        [  101,  2028,  1997,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = dfsampled['encoded'][:10].tolist()\n",
    "torch.tensor(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    hidden_states = model(torch.tensor(lst))\n",
    "    output = hidden_states[1][-1][:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.60967004,  0.49433842, -0.36431038, ..., -0.35125175,\n",
       "         0.7825085 , -0.73858076],\n",
       "       [-0.7300008 ,  0.1914191 , -0.42763206, ..., -0.39265317,\n",
       "         0.79416597, -0.43778157],\n",
       "       [-0.94352895,  0.36732632, -0.55836064, ..., -0.343816  ,\n",
       "         0.8057951 , -0.54308707],\n",
       "       ...,\n",
       "       [-0.74842674,  0.3906517 , -0.55189127, ..., -0.3108351 ,\n",
       "         0.81164694, -0.49274832],\n",
       "       [-0.8117502 ,  0.31144175, -0.5445946 , ..., -0.21613792,\n",
       "         0.89696753, -0.38428256],\n",
       "       [-0.6234377 , -0.02318926, -0.50432396, ..., -0.19832934,\n",
       "         0.9186022 , -0.64537215]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
